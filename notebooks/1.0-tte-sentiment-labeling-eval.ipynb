{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Labeling Exploration\n",
    "\n",
    "Getting to know sentiment labels. Especially to check consistency of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, string, pickle, random\n",
    "sys.path.append(\"..\")\n",
    "from config import credentials\n",
    "import dropbox\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 23\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1342, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_dbx = dropbox.DropboxTeam(credentials.dropbox_team_access_token)\n",
    "team_root = team_dbx.with_path_root(dropbox.common.PathRoot.namespace_id(\n",
    "    credentials.dropbox_team_namespace_id))\n",
    "user_dbx = team_root.as_user(credentials.dropbox_team_member_id)\n",
    "\n",
    "data_path = \"/Data/CSVData\"\n",
    "fpath = os.path.join(data_path, \"TestData\", \"forSentAnalysis.csv\")\n",
    "\n",
    "_, res = user_dbx.files_download(fpath)\n",
    "labeled_data = pd.read_csv(res.raw)\n",
    "labeled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating schema: {0: \"positive\", 10: \"neutral\", 20: \"negative\", 30: \"offensive\", -2: \"notAssessable\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unnecessary cols and rename them uniformly\n",
    "labeled_data.drop([\"id\", \"Level\", \"Topic\", \"sentiment\"], axis=1, inplace=True)\n",
    "labeled_data.columns = [\"source\", \"text\", \"rating\"]\n",
    "\n",
    "# Remove not assessable\n",
    "labeled_data = labeled_data[labeled_data.rating != -2]\n",
    "\n",
    "# Remove empty texts\n",
    "labeled_data.text.replace(\"\", np.nan, inplace=True)\n",
    "labeled_data.dropna(subset=[\"text\"], inplace=True)\n",
    "labeled_data = labeled_data[~labeled_data.text.str.isspace()]\n",
    "\n",
    "# Remove duplicated texts\n",
    "labeled_data.drop_duplicates(subset=[\"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "# Remap rating labels\n",
    "new_rating_schema = {0: 0, 10: 0, 20: 1, 30: 1}\n",
    "labeled_data.rating = labeled_data.rating.map(new_rating_schema)\n",
    "\n",
    "labeled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New rating schema: {0: \"non-negative\", 1: \"negative\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    546\n",
       "1    507\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_tokenize import spacy_tokenizer, plain_tokenizer\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=plain_tokenizer)\n",
    "\n",
    "X = count_vect.fit_transform(labeled_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function plain_tokenizer at 0x7fa3252a0290>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=plain_tokenizer)\n",
    "tfidf.fit(labeled_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labeled_data[\"kmeans\"] = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[538,   8],\n",
       "       [496,  11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labeled_data.rating, labeled_data.kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.99      0.68       546\n",
      "           1       0.58      0.02      0.04       507\n",
      "\n",
      "    accuracy                           0.52      1053\n",
      "   macro avg       0.55      0.50      0.36      1053\n",
      "weighted avg       0.55      0.52      0.37      1053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labeled_data.rating, labeled_data.kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=2, random_state=random_state)\n",
    "\n",
    "topic_distr = lda.fit_transform(X)\n",
    "\n",
    "labeled_data[\"topic\"] = topic_distr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200, 346],\n",
       "       [229, 278]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labeled_data.rating, labeled_data.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.37      0.41       546\n",
      "           1       0.45      0.55      0.49       507\n",
      "\n",
      "    accuracy                           0.45      1053\n",
      "   macro avg       0.46      0.46      0.45      1053\n",
      "weighted avg       0.46      0.45      0.45      1053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labeled_data.rating, labeled_data.topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nonnegative = labeled_data[labeled_data.rating == 0]\n",
    "X_negative = labeled_data[labeled_data.rating == 1]\n",
    "\n",
    "tfidf_nonnegative = tfidf.transform(X_nonnegative.text)\n",
    "tfidf_negative = tfidf.transform(X_negative.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg sim of all posts: 0.032144036401833546\n",
      "Avg sim negative posts: 0.018353174984894714\n",
      "Avg sim non-negative posts: 0.015136336853201173\n"
     ]
    }
   ],
   "source": [
    "cos_similarity = np.dot(tfidf_negative, tfidf_nonnegative.T).A\n",
    "avg_sim = np.diag(cos_similarity).mean()\n",
    "\n",
    "cos_similarity_neg = np.dot(tfidf_negative, tfidf_negative.T).A\n",
    "cos_similarity_nonneg = np.dot(tfidf_nonnegative, tfidf_nonnegative.T).A\n",
    "\n",
    "neg_avg_sim = np.tril(cos_similarity_neg, -1).mean()\n",
    "nonneg_avg_sim = np.tril(cos_similarity_nonneg, -1).mean()\n",
    "\n",
    "print(\"Avg sim of all posts:\", avg_sim)\n",
    "print(\"Avg sim negative posts:\", neg_avg_sim)\n",
    "print(\"Avg sim non-negative posts:\", nonneg_avg_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
