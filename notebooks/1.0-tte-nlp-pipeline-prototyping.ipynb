{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline Prototyping\n",
    "\n",
    "Load and preprocess FB & TW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, string\n",
    "sys.path.append(\"..\")\n",
    "from config import credentials\n",
    "import dropbox\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append(\"../data/external/nltk_data\")\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Data/CSVData\"\n",
    "\n",
    "fb_posts_path = os.path.join(data_path, \"FBPolTimeLines.csv\")\n",
    "fb_comments_path = os.path.join(data_path, \"FBUserComments.csv\")\n",
    "twitter_posts_path = os.path.join(data_path, \"TwPolTimeLines.csv\")\n",
    "twitter_comments_path = os.path.join(data_path, \"TwUserComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_dbx = dropbox.DropboxTeam(credentials.dropbox_team_access_token)\n",
    "team_root = team_dbx.with_path_root(dropbox.common.PathRoot.namespace_id(\n",
    "    credentials.dropbox_team_namespace_id))\n",
    "user_dbx = team_root.as_user(credentials.dropbox_team_member_id)\n",
    "\n",
    "_, res = user_dbx.files_download(fb_posts_path)\n",
    "fb_posts = pd.read_csv(res.raw)\n",
    "fb_posts[\"media_source\"] = \"fb\"\n",
    "_, res = user_dbx.files_download(fb_comments_path)\n",
    "fb_comments = pd.read_csv(res.raw)\n",
    "fb_comments[\"media_source\"] = \"fb\"\n",
    "\n",
    "_, res = user_dbx.files_download(twitter_posts_path)\n",
    "twitter_posts = pd.read_csv(res.raw)\n",
    "twitter_posts[\"media_source\"] = \"tw\"\n",
    "_, res = user_dbx.files_download(twitter_comments_path)\n",
    "twitter_comments = pd.read_csv(res.raw)\n",
    "twitter_comments[\"media_source\"] = \"tw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (172047, 41)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.concat([fb_posts, fb_comments, twitter_posts, twitter_comments], sort=False)\n",
    "assert raw_data.shape[0] == (fb_posts.shape[0] + fb_comments.shape[0] +\n",
    "                             twitter_posts.shape[0] + twitter_comments.shape[0]), \"False samples count\"\n",
    "\n",
    "print(\"Shape:\", raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows w/o text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed rows: 3533\n"
     ]
    }
   ],
   "source": [
    "data = raw_data.dropna(subset=[\"text\"], how=\"all\")\n",
    "print(\"Removed rows:\", raw_data.shape[0] - data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = [\"userID\", \"id\", \"replyToID\", \"replyToUser\", \"origPost\", \"Level\", \"Sampled\",\n",
    "               \"CommentsAvail\", \"RepAvail\", \"FileNum\", \"verified\", \"quoted_text\",\n",
    "               \"quoted_screen_name\", \"quoted_status_id\", \"quoted_favorite_count\",\n",
    "               \"quoted_retweet_count\", \"quoted_verified\", \"retweet_text\", \"retweet_screen_name\",\n",
    "               \"retweet_status_id\", \"retweet_favorite_count\", \"retweet_retweet_count\", \"retweet_verified\",\n",
    "               \"source\", \"replyLevel\"]\n",
    "\n",
    "data_cleaned = data.drop(remove_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text:\n",
    "* Lowercase ?\n",
    "* Numbers ?\n",
    "* Punctuation ?\n",
    "    * Mentions\n",
    "    * Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Clean text string \"\"\"\n",
    "    lowercased = text.lower()\n",
    "    punct_removed = lowercased.translate(str.maketrans(\"\", \"\", string.punctuation)) #FIXME lower quotation marks\n",
    "    num_replaced = re.sub(r\"\\b\\d+\\b\", \"NUM\", punct_removed)\n",
    "    return num_replaced\n",
    "\n",
    "data_cleaned[\"cleaned_text\"] = data_cleaned.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[\"tokens\"] = data_cleaned.cleaned_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump cleaned dataset\n",
    "\n",
    "To use for Sentiment Analysis and Topic Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = os.path.join(data_path, \"Test_CleanedTextDF.csv\")\n",
    "# user_dbx.files_upload(bytes(data_cleaned.to_csv(index=False), \"utf-8\"),\n",
    "#                       dump_path, mode=dropbox.files.WriteMode.overwrite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
