{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline Prototyping\n",
    "\n",
    "Load and preprocess FB & TW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append(\"/home/datadonk23/WDir/NLP/nltk_data\")\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/DATA/NRW2019 Dropbox/data 4good/CSVData\"\n",
    "\n",
    "fb_posts_path = os.path.join(data_path, \"FBPolTimeLines.csv\")\n",
    "fb_comments_path = os.path.join(data_path, \"FBUserComments.csv\")\n",
    "twitter_posts_path = os.path.join(data_path, \"TwPolTimeLines.csv\")\n",
    "twitter_comments_path = os.path.join(data_path, \"TwUserComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_posts = pd.read_csv(fb_posts_path)\n",
    "fb_posts[\"media_source\"] = \"fb\"\n",
    "fb_comments = pd.read_csv(fb_comments_path)\n",
    "fb_comments[\"media_source\"] = \"fb\"\n",
    "\n",
    "twitter_posts = pd.read_csv(twitter_posts_path)\n",
    "twitter_posts[\"media_source\"] = \"tw\"\n",
    "twitter_comments = pd.read_csv(twitter_comments_path)\n",
    "twitter_comments[\"media_source\"] = \"tw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (172047, 41)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.concat([fb_posts, fb_comments, twitter_posts, twitter_comments], sort=False)\n",
    "assert raw_data.shape[0] == (fb_posts.shape[0] + fb_comments.shape[0] +\n",
    "                             twitter_posts.shape[0] + twitter_comments.shape[0]), \"False samples count\"\n",
    "\n",
    "print(\"Shape:\", raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows w/o text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed rows: 3533\n"
     ]
    }
   ],
   "source": [
    "data = raw_data.dropna(subset=[\"text\"], how=\"all\")\n",
    "print(\"Removed rows:\", raw_data.shape[0] - data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text:\n",
    "* Lowercase ?\n",
    "* Numbers ?\n",
    "* Mentions\n",
    "* Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"cleaned_text\"] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"tokens\"] = data.cleaned_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump cleaned dataset\n",
    "\n",
    "To use for Sentiment Analysis and Topic Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = os.path.join(data_path, \"CleanedTextDF.csv\")\n",
    "#data.to_csv(dump_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
