{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype stopped: bad performance\n",
    "\n",
    "# Sentiment Analysis Prototyping\n",
    "\n",
    "Prototype code for rule-based sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, string\n",
    "sys.path.append(\"..\")\n",
    "from config import credentials\n",
    "import dropbox\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append(\"../data/external/nltk_data\")\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_dbx = dropbox.DropboxTeam(credentials.dropbox_team_access_token)\n",
    "team_root = team_dbx.with_path_root(dropbox.common.PathRoot.namespace_id(\n",
    "    credentials.dropbox_team_namespace_id))\n",
    "user_dbx = team_root.as_user(credentials.dropbox_team_member_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/Data/CSVData\"\n",
    "test_fpath = os.path.join(data_path, \"TestData\", \"forSentAnalysis.csv\")\n",
    "\n",
    "_, res = user_dbx.files_download(fpath)\n",
    "test_data = pd.read_csv(res.raw)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS total (incl. inflections): 16716\n",
      "NEG total (incl. inflections): 18217\n"
     ]
    }
   ],
   "source": [
    "sentiws_path = \"../data/external/SentiWS_v2.0\"\n",
    "positive_fpath = os.path.join(sentiws_path, \"SentiWS_v2.0_Positive.txt\")\n",
    "negative_fpath = os.path.join(sentiws_path, \"SentiWS_v2.0_Negative.txt\")\n",
    "\n",
    "# POSITIVE words\n",
    "positive = pd.read_csv(positive_fpath, sep=\"\\t\", names=[\"word_pos\", \"polarity\", \"inflections\"])\n",
    "positive[[\"word\", \"pos\"]] = positive.word_pos.str.split(\"|\", expand=True)\n",
    "positive = positive[[\"word\", \"polarity\", \"pos\", \"inflections\"]]\n",
    "pos_total = positive.word.count() + positive.inflections.str.split(\",\").dropna().apply(lambda x: len(x)).sum()\n",
    "print(\"POS total (incl. inflections):\", pos_total)\n",
    "\n",
    "# NEGATIVE words\n",
    "negative = pd.read_csv(negative_fpath, sep=\"\\t\", names=[\"word_pos\", \"polarity\", \"inflections\"])\n",
    "negative[[\"word\", \"pos\"]] = negative.word_pos.str.split(\"|\", expand=True)\n",
    "negative = negative[[\"word\", \"polarity\", \"pos\", \"inflections\"]]\n",
    "neg_total = negative.word.count() + negative.inflections.str.split(\",\").dropna().apply(lambda x: len(x)).sum()\n",
    "print(\"NEG total (incl. inflections):\", neg_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiWS lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexicon(polarity_df, lexicon):\n",
    "    \"\"\" Makes lexicon of pos/neg words with corresponding polarity score. \n",
    "    Util func: Appends words and inflections with \n",
    "    corresponding polarity value to lexicon dict.\n",
    "    \"\"\"\n",
    "    for _, row in polarity_df.iterrows():\n",
    "        lexicon[row[\"word\"].lower()] = row[\"polarity\"]\n",
    "        if row[\"inflections\"] is not np.nan:\n",
    "            words = row[\"inflections\"].split(\",\")\n",
    "            for word in words:\n",
    "                lexicon[word.lower()] = row[\"polarity\"]\n",
    "    return lexicon\n",
    "\n",
    "lexicon = {}\n",
    "lexicon = make_lexicon(positive, make_lexicon(negative, lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cleansing and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Util: Cleans text string.\n",
    "    > Lowercase string\n",
    "    > Replace game scores with \"GAME_SCORE\" placeholder\n",
    "    > Punctuation removal\n",
    "    > Replace numbers with \"NUM\" placeholder\n",
    "    \"\"\"\n",
    "    lowercased = text.lower()\n",
    "    scores_removed = re.sub(r\"(\\d+) ?(-|:) ?(\\d+)\", \"GAME_SCORE \", lowercased)\n",
    "    punctuations = string.punctuation + \"„\" + \"”\"\n",
    "    punct_removed = scores_removed.translate(str.maketrans(\"\", \"\",\n",
    "                                                           punctuations))\n",
    "    num_replaced = re.sub(r\"\\b\\d+\\b\", \"NUM\", punct_removed)\n",
    "\n",
    "    return num_replaced\n",
    "\n",
    "test_data[\"cleaned_txt\"] = test_data.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"tokens\"] = (test_data.cleaned_txt.apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter samples with pos/neu/neg sentiment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(845, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_dict = {0: \"positive\", 10: \"neutral\", 20: \"negative\", 30: \"offensive\", -2: \"notAssessable\"}\n",
    "\n",
    "test_data[\"rating\"] = test_data.replace({\"Rating\": ratings_dict}).Rating.astype(str)\n",
    "\n",
    "test_subset = test_data.loc[(test_data.rating == \"positive\") |\n",
    "                            (test_data.rating == \"neutral\") |\n",
    "                            (test_data.rating == \"negative\")]\n",
    "test_subset = test_subset.copy()\n",
    "test_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Assignment\n",
    "\n",
    "Rule-based approach, using SentiWS v2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy: Summing polarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sentiment_polar(tokens, lexicon):\n",
    "    \"\"\" Computes sentiment score by summing polarity scores \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in lexicon.keys():\n",
    "            score += lexicon[word]\n",
    "            \n",
    "    return score\n",
    "\n",
    "test_subset[\"sentiment_score_polar\"] = test_subset.tokens.apply(lambda x: assign_sentiment_polar(x, lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "Policy: 0 -> neutral (& kW); < 0 -> negative; > 0 -> positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset[\"sentiment_polar\"] = test_subset.sentiment_score_polar.apply(lambda x: \"negative\" if x < 0\n",
    "                                                                         else \"positive\" if x > 0 \n",
    "                                                                         else \"neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy: Frequency of pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sentiment_freq(tokens, lexicon):\n",
    "    \"\"\" Computes sentiment score by summing-up count of pos/neg words \"\"\"\n",
    "    neg, pos = 0, 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in lexicon.keys():\n",
    "            if lexicon[word] > 0:\n",
    "                pos += 1\n",
    "            else:\n",
    "                neg += 1\n",
    "    \n",
    "    if pos > neg:\n",
    "        return \"positive\"\n",
    "    elif neg > pos:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "Policy: same count or 0 -> neutral; more pos -> positive; more neg -> negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset[\"sentiment_freq\"] = test_subset.tokens.apply(lambda x: assign_sentiment_freq(x, lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ratings_dict = [\"positive\", \"neutral\", \"negative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy: Summing polarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>85</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>neutral</td>\n",
       "      <td>118</td>\n",
       "      <td>28</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          positive  neutral  negative\n",
       "True                                 \n",
       "positive        85       19        34\n",
       "neutral        118       28       165\n",
       "negative        89       31       276"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_polar = confusion_matrix(test_subset.rating, test_subset.sentiment_polar, labels=labels)\n",
    "\n",
    "cm_df_polar = pd.DataFrame(cm_polar, columns=labels, index=labels)\n",
    "cm_df_polar.index.name = \"True\"\n",
    "cm_df_polar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy: Frequency of pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>96</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>neutral</td>\n",
       "      <td>144</td>\n",
       "      <td>69</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>123</td>\n",
       "      <td>93</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          positive  neutral  negative\n",
       "True                                 \n",
       "positive        96       23        19\n",
       "neutral        144       69        98\n",
       "negative       123       93       180"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_freq = confusion_matrix(test_subset.rating, test_subset.sentiment_freq, labels=labels)\n",
    "\n",
    "cm_df_freq = pd.DataFrame(cm_freq, columns=labels, index=labels)\n",
    "cm_df_freq.index.name = \"True\"\n",
    "cm_df_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy: Summing polarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.29      0.62      0.40       138\n",
      "     neutral       0.36      0.09      0.14       311\n",
      "    negative       0.58      0.70      0.63       396\n",
      "\n",
      "    accuracy                           0.46       845\n",
      "   macro avg       0.41      0.47      0.39       845\n",
      "weighted avg       0.45      0.46      0.41       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_subset.rating, test_subset.sentiment_polar, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy: Frequency of pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.26      0.70      0.38       138\n",
      "     neutral       0.37      0.22      0.28       311\n",
      "    negative       0.61      0.45      0.52       396\n",
      "\n",
      "    accuracy                           0.41       845\n",
      "   macro avg       0.41      0.46      0.39       845\n",
      "weighted avg       0.46      0.41      0.41       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_subset.rating, test_subset.sentiment_freq, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Both implemented rule-based approaches perform bad. The one with the policy of summing-up polarity scores from SentiWS performs slightly better, esp. at the task of classifying negative sentiment. Nonetheless both approaches shouldn't be used in produtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
