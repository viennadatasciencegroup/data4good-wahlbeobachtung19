{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype finished.\n",
    "\n",
    "# Topic Prototyping\n",
    "\n",
    "Prototype code for Topic Modeling of Posts. Objective is to find the best topic model for this data by visually inspect most promising models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string, pickle, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from joblib import dump, load\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/DATA/NRW2019 Dropbox/data 4good/CSVData\"\n",
    "model_path = \"../models\"\n",
    "figures_path = \"../reports/figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_fpath = os.path.join(data_path, \"PolPosts.csv\")\n",
    "\n",
    "raw_posts = pd.read_csv(posts_fpath)\n",
    "print(\"Posts\", raw_posts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary cols and rename them uniformly\n",
    "posts_cols = raw_posts.columns.to_list()\n",
    "posts_cols.remove(\"text\")\n",
    "posts_cols.remove(\"textID\") # keep these cols\n",
    "raw_posts.drop(posts_cols, axis=1, inplace=True)\n",
    "raw_posts.columns = [\"text\", \"textID\"]\n",
    "\n",
    "corpus = raw_posts.copy()\n",
    "\n",
    "# Remove empty texts\n",
    "corpus.text.replace(\"\", np.nan, inplace=True)\n",
    "corpus.dropna(subset=[\"text\"], inplace=True)\n",
    "corpus = corpus[~corpus.text.str.isspace()]\n",
    "\n",
    "# Remove duplicated texts\n",
    "corpus.drop_duplicates(subset=[\"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicated texts (after cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.text.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicated textIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.textID.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "TFIDF + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_tokenize import topic_tokenizer\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True, tokenizer=topic_tokenizer, max_features=40000) # total tokens 43656\n",
    "tfidf_v = tfidf.fit_transform(data.text)\n",
    "\n",
    "dump(tfidf, os.path.join(model_path, \"topic_vectorizer\", \"tfidf.joblib\"))\n",
    "dump(tfidf_v, os.path.join(model_path, \"topic_vectorizer\", \"tfidf_v.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"lda_20\": LatentDirichletAllocation(n_components=20, n_jobs=1, random_state=random_state, verbose=1),\n",
    "    \"lda_25\": LatentDirichletAllocation(n_components=25, n_jobs=1, random_state=random_state, verbose=1),\n",
    "    \"lda_30\": LatentDirichletAllocation(n_components=30, n_jobs=1, random_state=random_state, verbose=1),\n",
    "    \"lda_40\": LatentDirichletAllocation(n_components=40, n_jobs=1, random_state=random_state, verbose=1),\n",
    "    \"lda_50\": LatentDirichletAllocation(n_components=50, n_jobs=1, random_state=random_state, verbose=1),\n",
    "    \"lda_80\": LatentDirichletAllocation(n_components=80, n_jobs=1, random_state=random_state, verbose=1),\n",
    "    \"lda_100\": LatentDirichletAllocation(n_components=100, n_jobs=1, random_state=random_state, verbose=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path = os.path.join(model_path, \"topic_lda\")\n",
    "\n",
    "for model in models:\n",
    "    print(\"Fitting\", model)\n",
    "    models[model].fit(tfidf_v)\n",
    "    print(\"Dump\", model, \"\\n\")\n",
    "    dump(models[model], os.path.join(lda_path, model + \".joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(model, name):\n",
    "    \"\"\" Prints Log-Likelihoodk and Perplexity scors of model. \"\"\"\n",
    "    print(\"Scores for\", name)\n",
    "    print(\"Log Likelihood:\", model.score(tfidf_v))\n",
    "    print(\"Perplexity:\", model.perplexity(tfidf_v), \"\\n\")\n",
    "    \n",
    "    \n",
    "for model in models:\n",
    "    print_scores(models[model], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_path = os.path.join(figures_path, \"topics\")\n",
    "\n",
    "for model in models:\n",
    "    if model == \"lda_100\": # skip due to performance constraints (out of memory)\n",
    "        pass\n",
    "    else:\n",
    "        p = None\n",
    "        p = pyLDAvis.sklearn.prepare(models[model], tfidf_v, tfidf, mds=\"mmds\")#\"tsne\")\n",
    "        pyLDAvis.save_html(p, os.path.join(viz_path, \"topics_\" + model + \".html\"))\n",
    "        print(\"topics_\" + model + \" persisted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Visual inspection and interpretation of models leads to 3 most promising candidates.  \n",
    "These are:\n",
    "* 25 topics - best dense representation.\n",
    "* 30 topics - best seperation.\n",
    "* 40 topics - best interpretability.\n",
    "\n",
    "As resulting topics of the model consisting of 40 components are best interpretable and even the dichotomy of left- & right-wing topics can be seperated (along PC1 axis of t-SNE plot), this model has been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
